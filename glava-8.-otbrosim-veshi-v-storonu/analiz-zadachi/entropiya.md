# 8.3.2.Энтропия

Существует другой тип процесса для создания деревьев решений и разделения данных на категории. Он называется моделью энтропии или получения информации. Энтропия - это измерение количества беспорядка в выборке предоставленных данных. Мы также можем назвать этот процесс получением информации, так как мы измеряем, насколько сильно каждый критерий влияет на принадлежность объекта к какому-либо классу.

Формула для энтропии - это отрицательная функция log по основанию 2, которая все еще в основном ориентируется на вероятность того, что класс принадлежит конкретной популяции, а это просто количество особей, принадлежащих к каждому классу, деленная на общее количество особей в выборке:

$$
Entropy = Σ_(i=1)^c▒[-p_i*log2(p_i)]
$$

  
Чтобы заменить энтропию в качестве группового критерия в нашей программе, нам нужно всего лишь изменить одну строку:

```text
dTree = tree.DecisionTreeClassifier(criterion ="entropy")
```

Результат продемонстрирован на следующей диаграмме:

![](../../.gitbook/assets/image%20%284%29.png)

  
Вы можете заметить, что энтропия начинается с 2,55 для всей нашей группы и уменьшается до 0 на узловых листья \(концах ветвей\). Мы можем проверить, что у нас есть семь классификаций, но вы могли увидеть что метод энтропии выбирает другой критерий, нежели метод Джини. Например, классификатор Джини начинался с длины, а классификатор энтропии начинается с материала. Метод энтропии также выбрал шум \(независимо от того, шумит игрушка или нет\) и правильно выбрал, что единственными игрушками, которые издают шум, были игрушечные музыкальные инструменты и самолеты, которые имеют электронные звуковые ящики, издающие звуки самолетов.

Однако есть одна вещь, которая вызывает некоторое беспокойство. Есть два блока, которые показывают критерий, разделяющий игрушки по материалу с величиной менее 2,5 и более. Материал является дискретной величиной. Мы можем создать список материалов и запустить его через наш отсортированный \(set \(list\)\) процесс, чтобы получить уникальные значения в отсортированном порядке:

```text
['ceramic', 'fur', 'metal', 'plastic', 'wood']
```

  
Таким образом, материальная ценность 2,5 или меньше будет либо у керамики, либо у меха. Мех и керамика не имеют ничего общего, кроме того, где они находятся в алфавите. Это довольно тревожные отношения, которые являются артефактом того, что мы закодировали наши данные как последовательный набор чисел. Это подразумевает отношения и группировку, которых на самом деле не существует. Как мы можем это исправить?

На самом деле, есть процесс для решения именно такой проблемы. Эта техника широко используется в программах ИИ и является обязательным инструментом для работы с классификацией, либо здесь, в разделе деревьев решений, либо с нейронными сетями. Этот инструмент имеет странное название «унитарный код» \(one hot encoding\).

