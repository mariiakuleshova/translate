# 8.3.1.2.Самостоятельная классификация деревьев решений и инструменты ИИ

  
Раньше мы, люди истории, создавали эти деревья решений вручную. Мы даже не обсуждали, как написать программу, позволяющую роботу использовать деревья чтобы принимать решения. Разве не было бы намного лучше, если бы компьютер делал всю сложную часть в процессе создания дерева вместо нас: определял ветви и помечал узлы? Это именно то, чем мы будем заниматься в этом разделе.

Давайте рассмотрим проблему классификации игрушек. Мы могли бы придумать более эффективного робота, который каким-то образом сортирует игрушки, а не просто складывает их в коробку. В идеальном мире, из 20 игрушек, мы бы разделили кучу игрушек поровну - 10 и 10. Допустим, наш критерий для классификации - это длина:  половина игрушек до шести дюймов в длину и половина длиннее. Тогда было бы также идеально, если бы какая-то другая характеристика делила каждую из этих куч по 10 игрушек пополам – то есть четыре группы по пять игрушек. Пусть это будет цвет - у нас есть пять красных игрушек, пять синих игрушек, пять зеленых игрушек и пять желтых игрушек. Очень хорошо и организованно. Вы можете признать, что мы делаем то, что делают биологи при классификации новых виды - мы создаем таксономию. Теперь мы выбираем другой атрибут, который отделяет игрушки на еще более мелкие группы - это может быть, характеристика того, какой тип у этой игрушки или какой у нее размер колес. Я думаю, вы поняли логику.

Давайте рассмотрим пример.

Теперь было бы здорово, если бы мы могли перечислить все игрушки и все атрибуты в таблице, и пусть компьютер выяснит, сколько групп и каких видов. Мы могли бы создать таблицу, подобную этой:

| Тип | Длина | Ширина | Вес | Цвет | Количество колес | Звуки | Уровень жесткости | Материал | Глаза | Имя игрушки |
| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |
| машинка | 3 | 1 | 35 | красный | 4 | 0 | жесткий | металл | 0 | HotWheels |
| машинка | 3 | 1 | 35 | оранжевый | 4 | 0 | жесткий | металл | 0 | HotWheels |
| машинка | 3 | 1 | 35 | голубой | 4 | 0 | жесткий | металл | 0 | HotWheels |
| машинка | 3 | 1 | 35 | голубой | 4 | 0 | жесткий | металл | 0 | HotWheels |
| машинка | 3 | 1 | 35 | белый | 4 | 0 | жесткий | металл | 0 | HotWheels |
| мягкая игрушка | 5 | 5 | 50 | белый | 0 | 0 | очень мягкий | мех | 2 | Plush |
| мягкая игрушка | 7 | 5 | 55 | коричневый | 0 | 0 | очень мягкий | мех | 3 | Plush |
| Для активных игр | 2 | 4 | 80 | серый | 0 | 0 | жесткий | метал | 0 | пружинка |
| Конструктор | 2 | 2 | 125 | деревянный | 0 | 0 | жесткий | дерево | 0 | деревянный блок 2х2 |
| Конструктор | 2 | 2 | 75 | деревянный | 0 | 0 | жесткий | дерево | 0 | треугольный деревянный блок |
| Конструктор | 4 | 2 | 250 | деревянный | 0 | 0 | жесткий | дерево | 0 | деревянный блок 4х2 |
| Посуда | 3 | 3 | 79 | голубой | 0 | 0 | жесткий | керамика | 0 | чайный сервиз |
| Самолет | 7 | 5 | 65 | белый | 4 | 1 | жесткий | пластик | 0 | космический корабль |
| Самолет | 13 | 7 | 500 | зеленый | 8 | 1 | жесткий | пластик | 0 | Thunderbird 2 |
| Машинка | 5 | 1 | 333 | желтый | 6 | 1 | жесткий | металл | 0 | школьный автобус |
| Музыкальная | 12 | 4 | 130 | деревянный | 0 | 2 | жесткий | дерево | 0 | гитара |
| Музыкальная | 5 | 2 | 100 | желтый | 0 | 1 | жесткий | пластик | 0 | микрофон |
| Музыкальная | 4 | 4 | 189 | белый | 0 | 2 | жесткий | дерево | 0 | барабан |

  
У нас есть проблема, которую мы должны решить. Мы будем использовать классификатор дерева решений, который поставляется с пакетом Scikit-Learn Python, который называется DecisionTreeClassifier. Эта программа не может использовать строки в качестве входных данных. Нам придется конвертировать все наши строковые данные в какую-то числовую форму. К счастью, люди из библиотеки Scikit предоставили нам функции с этой целью. На самом деле, они предоставили несколько кодировок функций, которые преобразуют строки в числа. Функция, которую мы будем использовать, называется LabelEncoder. Эта функция берет массив строк и преобразует его в перечисляемый набор целых чисел.

Мы можем взять нашу первую колонку, которая содержит тип игрушки. Мы должны превратить все типы в какие-то цифры.

LabelEncoder преобразует столбец в нашей таблице данных, заполненный строками.

Столбец типа игрушки показан в следующем коде:

```text
['машинка' 'машинка' 'машинка
' 'машинка' 'машинка' 'мягкая игрушка' 'мягкая игрушка' 'для активных игр' 'конструктор' 'конструктор' 'конструктор' 'посуда'
'самолет' 'самолет' 'машинка' 'музыкальная' 'музыкальная' 'музыкальная']
```

Он преобразует его в метку закодированного типа игрушки:

```text
[3 3 3 3 3 6 6 0 2 2 2 4 1 1 3 5 5 5]
```

Вы можете видеть, что везде, где сказано, машина, у нас теперь есть номер 3. 6 = мягкая игрушка, 0 = для активных игр и так далее. Почему нечетная нумерация? Кодер сначала сортирует строки в алфавитном порядок.

Мы будем погружаться в процесс прямо отсюда, чтобы создать программу классификации. Я объясню все по пути, так что по крайней мере следуйте вместе с кодом.

Вот наша программа классификации дерева решений:

```text
# decision tree classifier
# author: Francis X Govers III
#
# example from book "Artificial Intelligence for Robotics"
#
```

  
Мы можем импортировать наши библиотеки, которые мы будем использовать. Существует дополнительная библиотека под названием graphviz, который полезен для рисования деревьев решений. Вы можете установить его с помощью pip install graphviz. Мы будем использовать пакет Pandas, который предоставляет много инструментов обработки таблиц данных:

```text
from sklearn import tree
import numpy as np
import pandas as pd
import sklearn.preprocessing as preproc
import graphviz
```

  
Наш первый шаг - прочитать наши данные. Я создал свою таблицу в Microsoft Excel и экспортировал ее как формат CSV \(comma separated values\)\). Это позволяет нам читать напрямую с заголовками столбцов в файле данных. Я распечатаю форму и размер файла данных для справки. Моя версия файла имеет 18 строк и 11 столбцов. Последний столбец - только примечание для меня о фактическом названии каждой игрушки. Мы не будем использовать последний столбец ни для чего. Мы сделаем классификатор, который разделит игрушки по типам:

```text
toyData = pd.read_csv("toy_classifier_tree.csv")
print ("Data length ",len(toyData))
print ("Data Shape ",toyData.shape)
```

  
Теперь мы можем начать строить наш классификатор дерева решений. Сначала мы создаем экземпляр Объект DecisionTreeClassifer. Существует два разных типа DTS \(decision tree classification\) алгоритма на выбор. Мы собираемся использовать Джини. Что такое Джини? Потребовалось немного покопаться, чтобы дать ответ, но коэффициент Джини был разработан в 1912 году итальянским статистиком Коррадо Джини в его статье «Variabilita e Mutabilita.». Этот коэффициент, или индекс, измеряет количество неравенств в группе чисел. Нулевое значение означает все члены группы одинаковы. Например, если бы у нас была группа игрушечных машин, которые были все одинакового размера и все красные, то индекс Джини группы будет 0. Если члены группы все разные, то число Джини ближе к 1. Индекс Джини равен 1 минус сумма квадратов вероятности того, что предмет находится в этом классе. У нас четыре игрушки машинки из 18 игрушек, поэтому вероятность того, что игрушечный автомобиль окажется в группе, составляет 4/18 или 0,222. Дерево решений будет продолжать делить классы до тех пор, пока индекс Джини группы не будет равен 0:

```text
dTree = tree.DecisionTreeClassifier(criterion ="gini")
```



Нам нужно выделить значения в нашей таблице данных. Данные в первом столбце, который называется столбец 0 \(ноль\) в Python, наши классификационные метки. Мы должны вытащить их отдельно, так как они используются для разделения игрушек на классы. Из нашей предыдущей работы с нейронными сетями, это будут наши результаты или метки данных, которые мы использовали в других процессах машинного обучения. Мы будем обучать наш классификатор, чтобы предсказать класс игрушки на основе атрибутов в таблице \(размер, вес, цвет и т.д.\). Мы используем нарезку, чтобы тянуть данные из Pandas table. Наша таблица данных Pandas называется toyData. Если мы хотим получить запись из таблицы, нам нужно запросить toyData.values, который будет возвращен как 2D массив:

```text
dataValues=toyData.values[:,1:10]
classValues = toyData.values[:,0]
```

  
Это наш кодировщик меток, о котором мы говорили - он будет преобразовывать строки в наших данных в числа. Например, такие цвета, как «красный», «зеленый» и «синий» будут преобразованы в числа как 0,1 и 2. Первый элемент, который будет закодирован, является списком значений класса, который мы используем для того, чтобы пометить данные. Мы используем функцию LableEncoder.fit \(\), чтобы сделать формулу для преобразования строк в числа, а затем функцию LabelEncoder.transform \(\) для того, чтобы применить ее. Обратите внимание, что fit \(\) не производит вывод. Наконец, нам нужно сделать текстовую строку и список закодированных номеров, которые будут соотноситься с ней.

LabelEncoder будет сортировать строки по алфавиту и начинать их нумерацию от «А», игнорируя любые дубликаты. Если мы поместим в «машинка, машинка, машинка, блок, мягкая игрушка, самолет» мы получим в качестве кодировки «2,2,2,1,3,0», и мы должны знать, что самолет = 0, блок = 1, машинка = 2 и stuffed = 3. Нам нужно сгенерировать «кольцо декодера», чтобы соответствовать числам и текстовым описаниям, которые выглядят как «самолет, блок, машинка, мягкая игрушка». Мы дублируем функцию LabelEncoder, используя две функции в нашем списке имен классов в строковом формате. Мы используем функцию set \(\) для удаления дубликатов и функцию sorted \(\) для сортировки в правильном порядке. Теперь наша таблица сгенерирована в соответствие с LabelEncoder. Нам понадобится это позже:

```text
lencoder = preproc.LabelEncoder()
lencoder.fit(classValues)
classes = lencoder.transform(classValues)
classValues = list(sorted(set(classValues)))
```

  
Чтобы облегчить себе задачу, я создал функцию, которая автоматически определяет, в какие столбцы наших данных входят строки, и преобразует этих столбцы в числа. Мы начинаем, создав пустой список для хранения наших данных. Мы будем перебирать столбцы в наших данных и смотреть является ли первое значение данных строкой. Если это так, мы преобразуем весь этот столбец в числа, используя объект кодировщика меток \(lecoder\), который мы создали. Процесс кодировки состоит из двух частей. Мы вызываем lecoder.fit \(\), чтобы увидеть, сколько уникальных строк есть в нашем столбце и создать номер для каждой. Затем мы используем lecoder.transpose для того, чтобы вставить эти цифры в список:

```text
newData = []
for ii in range(len(dataValues[0]))
line = dataValues[:,ii]
if type(line[0])==str:
lencoder.fit(line)
line = lencoder.transform(line)
```

  
Теперь мы помещаем все данные обратно в список newData, но есть проблема – мы превратили все наши столбцы в ряды! Мы используем функцию transpose из numpy для исправления этой проблемы. Но подождите! У нас больше нет массива, так как мы превратили его в список, поэтому мы может разобрать его и снова собрать вместе. \(Вы не можете сделать это с пустым массивом - поверьте, я пробовал\):

```text
newData.append(line)
newDataArray = np.asarray(newData)
newDataArray = np.transpose(newDataArray)
```

  
Теперь вся наша предварительная обработка завершена, поэтому мы можем, наконец, вызвать реальную DecisionTreeClassifer. Она принимает два аргумента: сначала массив наших значений данных, и затем массив типов классов, на которые мы хотим, чтобы дерево решений делило наши группы. DecisionTreeClassifier определит, какие конкретные данные из таблицы полезны для предсказания, к какому классу относится одна из наших игрушек:

```text
dTree = dTree.fit(newDataArray,classes)
```

  
Вот и все - одна строка. Но подождите - мы хотим увидеть результаты. Если мы просто попробуем распечатать дерево решений получим следующее:

```text
DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,
max_features=None, max_leaf_nodes=None,
min_impurity_split=1e-07, min_samples_leaf=1,
min_samples_split=2, min_weight_fraction_leaf=0.0,
presort=False, random_state=None, splitter='best')
```

  
Это ничего не говорит нам; это описание объекта DecisionTreeClassifier. \(Он показывает нам все параметры, которые мы можем установить, вот почему я поместил его здесь.\) Итак, есть пакет, называемый graphviz, который очень хорош при печати деревьев решений. Давайте использовать его. Мы даже можем передать имена наших столбцов и классов в график. Последние два строки выводят график в виде файла .pdf и сохраняют его на жестком диске:

```text
c_data=tree.export_graphviz(dTree,out_file=None,feature_names=toyData.colum
ns, class_names=classValues, filled = True,
rounded=True,special_characters=True)
graph = graphviz.Source(c_data)
graph.render("toy_graph_gini")
```

И вот результат:

![](../../../.gitbook/assets/image%20%283%29.png)

  
Мы можем быстро проверить наше решение, посмотрев на нашу таблицу ввода и посмотрев на расположение объектов. 

Мы должны увидеть:

·        Шесть игрушечных машин

·        Три строительных блока

·        Один набор посуды

·        Одна игрушка для активных игр

·        Две мягкие игрушки

·        Три музыкальных инструмента

·        Два игрушечных самолета

И это действительно так.

Другое цифры, на которые можно посмотреть -  это индекс Джини. Поле верхнего уровня показывает, что индекс для всей группы имеет общее значение 0,8166, которое близко к 1 и показывает высокую степень неоднородности. По мере продвижения вниз по дереву числа Джини становятся все меньше и меньше до достижения 0 в каждой из указанных групп, что показывает, что элементы в этих группах разделяют схожие атрибуты.

Что этот график говорит нам? Прежде всего, мы можем выделить игрушечные машины только по одному атрибуту - ширина. Только игрушечные машинки имеют ширину менее 1,5 дюйма \(38 мм\). Нам не нужно смотреть на цвет или вес, или что-либо, кроме ширины, чтобы отделить все игрушечные машинки от всего остального. Мы видим, что у нас есть пять игрушечных машин из наших 17 игрушек, поэтому нам осталось 12  игрушек на классификацию. Наше следующее подразделение идет в длину. У нас есть семь игрушек длиной менее 4,5 дюймов \(11 см\) и пять длиннее. Из группы из пяти у двоих есть глаза, а у трех нет. Игрушки с глазами это две мягкие игрушки. Если вы следуете по дереву, ветви, которого ведут к игрушечным музыкальным инструментам: ширина&gt; 1,5 дюйма, длина&gt; 4,5 дюйма, без глаз. И они действительно больше, чем другие игрушки по длине и ширине, и у них нет глаз.

Ни один из других атрибутов не имеет значения с точки зрения классификации. Это означает, что такой атрибут, как цвет - плохой предсказатель того, к какому классу принадлежит игрушка - что очевидно. Другими полезными критериями являются количество колес, вес и длина. Этих данных достаточно для классификации всех наших игрушек в группы. Вы можете видеть, что индекс Джини каждого конечного узла действительно нуль. Признаюсь, я добавил некоторые дополнительные метки на график, чтобы сделать иллюстрация понятнее.

Итак, это упражнение было удовлетворительным - мы смогли создать автоматическое дерево решений на данных об игрушках, которое классифицировало наши игрушки. Мы даже можем использовать эти данные для классификации новой игрушки, и предсказать, к какому классу она может принадлежать. Если мы обнаружили, что эта новая игрушка нарушает классификацию каким-то образом, то нам нужно будет повторно запустить процесс классификации и сделать новую таблицу решений.

